Pipeline 1: Data Extraction
In the first step, the program connects to the Alpha Vantage API, which is a free online service that provides stock market data. We tell it which stock we want (for example, Amazon – symbol: AMZN) and ask for the daily closing prices for as many past days as possible. The code then downloads this information — each date and its closing price — and stores them in two lists: one for the dates and one for the prices. It reverses these lists so the data goes from oldest to newest (since the API sends them in reverse order). At the end of this step, we have a clean timeline of daily closing prices ready for processing.

Pipeline 2: Data Preparation
Now that we have the raw data, the next step is to prepare it for the LSTM model. Neural networks work best when numbers are small and consistent, so we first normalize the data — this means we scale all prices so that they’re roughly between -1 and +1.
Then, we create input–output pairs using a sliding window approach. Here’s what that means: we take the prices from the past 60 days (that’s one “window”) and use those as the input to predict the price on the next day (the 61st day). This helps the model learn patterns from history.
The whole dataset is then split into training data (80%) and validation data (20%). Training data teaches the model, and validation data tests how well it learned.
Finally, we convert these arrays into PyTorch datasets — a special format that the PyTorch library can easily work with. We wrap them into DataLoaders, which automatically feed small batches of data to the model during training.

Pipeline 3: Building the LSTM Model
This is where we define the neural network, specifically a type called LSTM (Long Short-Term Memory).
An LSTM is good at understanding sequences — like time series — because it can “remember” information from earlier time steps while processing later ones.
Here’s what each part of our model does:

Linear + ReLU layer: Before going into the LSTM, the input (closing prices) is passed through a small layer that changes its size and adds some non-linearity (ReLU activation). This helps the model learn complex patterns, not just straight lines.

LSTM layer: This is the heart of the network. It has 2 layers and 32 memory units in each layer. Each LSTM cell looks at one day’s price, remembers useful information, and forgets unimportant parts. Over 60 days, it builds a memory of how prices moved.

Dropout layer: During training, dropout randomly turns off some units (20% of them) to prevent the model from memorizing the data — this is called regularization.

Output layer: After the LSTM finishes reading the sequence, the last hidden outputs are flattened and passed to a final linear layer that predicts one number — the next day’s closing price (still in normalized form).
All layers are connected in a forward flow: input → linear + ReLU → LSTM → dropout → output. The model learns through repeated updates to its internal weights.

Pipeline 4: Training the LSTM
Once the model is built, we start training it — which means teaching it to minimize the difference between its predictions and the real prices.
For this, we use:

Loss function: Mean Squared Error (MSE), which measures how far the predicted price is from the actual price. Lower MSE means better performance.

Optimizer: Adam optimizer, a popular algorithm that adjusts the model’s weights automatically to reduce the loss efficiently.

Learning rate: Starts at 0.01 and becomes smaller every 40 epochs, allowing the model to learn big steps early and fine-tune later.
During each epoch (training round), the model sees all batches of training data, makes predictions, compares them with real values, and updates itself.
After each epoch, the model is also tested on the validation set (data it hasn’t seen before) to check if it’s learning general patterns and not just memorizing. The training and validation loss are printed every time so you can see if it’s improving.

Pipeline 5: Testing and Predicting
When training is finished, the model is ready to make predictions.
First, we let it predict prices for the training and validation sets again, to check how close its predictions are to the real prices. Then we convert those normalized predictions back into real price values (by undoing the normalization).
To measure how accurate the model is, we use MAPE (Mean Absolute Percentage Error) — it tells us the average percentage error between the true and predicted prices. For example, if MAPE = 4%, that means on average the model’s predictions are 4% off from the real prices.
Finally, the model is used to predict the next day’s closing price. It takes the most recent 60 days of data, processes them, and outputs the predicted closing price for “tomorrow.”
The code then shows several plots:

Actual vs predicted prices during training

Actual vs predicted prices during validation

A zoomed-in view for the recent days, ending with the “next day” forecast
At the end, it prints the predicted closing price for the next trading day.